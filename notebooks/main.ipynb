{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4730f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223e145",
   "metadata": {},
   "source": [
    "## Setup Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2919647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/mistral-7b-v0.1.Q5_K_M.gguf\",\n",
    "    #lib_path=\"../llama-cpp/libllama.so\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=80, # Keep this relatively low for Cypher generation\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Setup a potentially different LLM for final answer (optional, can be same)\n",
    "# For final answers, you might want a slightly higher max_tokens\n",
    "llm_final_answer = LlamaCpp(\n",
    "    model_path=\"../models/mistral-7b-v0.1.Q5_K_M.gguf\", # Using the same model for simplicity\n",
    "    #lib_path=\"../llama-cpp/libllama.so\",\n",
    "    temperature=0.2, \n",
    "    max_tokens=200, \n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f8ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"neo4jneo4j\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9c061",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4260ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CypherOutputParser:\n",
    "    def parse(self, text: str) -> str:\n",
    "        text = text.strip()\n",
    "        if \"```\" in text:\n",
    "            match = re.search(r'```(?:cypher)?\\n?(.*?)\\n?```', text, re.DOTALL)\n",
    "            if match:\n",
    "                text = match.group(1).strip()\n",
    "            else:\n",
    "                text = text.replace('```', '').strip()\n",
    "        prefixes = [\n",
    "            \"the answer is:\", \"here is the cypher query:\", \"cypher query:\",\n",
    "            \"cypher:\", \"query:\", \"answer:\", \"question:\"\n",
    "        ]\n",
    "        for prefix in prefixes:\n",
    "            if text.lower().startswith(prefix):\n",
    "                text = text[len(prefix):].strip()\n",
    "        lines = text.split('\\n')\n",
    "        cypher_lines = []\n",
    "        found = False\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if any(p in line.lower() for p in [\n",
    "                'explanation', 'note:', 'this query', 'the query',\n",
    "                'question:', 'schema:', 'generate only'\n",
    "            ]):\n",
    "                continue\n",
    "            if (found or any(line.upper().startswith(k) for k in\n",
    "                ['MATCH', 'CREATE', 'RETURN', 'WITH', 'MERGE', 'DELETE',\n",
    "                 'DETACH', 'SET', 'REMOVE', 'UNWIND', 'CALL', 'SHOW'])):\n",
    "                found = True\n",
    "                cypher_lines.append(line)\n",
    "        result = '\\n'.join(cypher_lines).strip()\n",
    "        result = re.sub(r'\\n+(This|The|Note:|Explanation:).*', '', result, flags=re.IGNORECASE | re.DOTALL)\n",
    "        return result\n",
    "\n",
    "def add_limit_to_cypher(cypher: str, limit: int = 10) -> str:\n",
    "    cypher_upper = cypher.upper()\n",
    "    if 'LIMIT' not in cypher_upper:\n",
    "        if 'ORDER BY' in cypher_upper:\n",
    "            parts = cypher.split('ORDER BY')\n",
    "            return f\"{parts[0].strip()} LIMIT {limit} ORDER BY {parts[1]}\"\n",
    "        else:\n",
    "            return f\"{cypher.strip()} LIMIT {limit}\"\n",
    "    return cypher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034be4e",
   "metadata": {},
   "source": [
    "## LangChain Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94ba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the graph schema\n",
    "schema = graph.get_schema\n",
    "\n",
    "# 2. Define the prompt template for LLM to generate Cypher\n",
    "cypher_generation_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Generate ONLY a Cypher query. No explanation, no markdown, no code blocks.\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\n",
    "Cypher:\"\"\"\n",
    ")\n",
    "\n",
    "# 3. Instantiate the parser\n",
    "cypher_parser = CypherOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950aab01",
   "metadata": {},
   "source": [
    "## Define a function to execute the query and format results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50f1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_and_format_cypher_query(input_dict: dict) -> dict:\n",
    "    cypher_query = input_dict[\"cypher_query\"]\n",
    "    original_question = input_dict[\"question\"] # Get original question\n",
    "    print(f\"\\nExecuting Cypher: {cypher_query}\")\n",
    "    try:\n",
    "        results = graph.query(cypher_query)\n",
    "        if results:\n",
    "            # Convert results to a string, handling potential list of dicts\n",
    "            formatted_results = \"\\n\".join([str(item) for item in results])\n",
    "            print(f\"Query results: {formatted_results}\")\n",
    "            return {\"question\": original_question, \"results\": formatted_results}\n",
    "        else:\n",
    "            return {\"question\": original_question, \"results\": \"No results found.\"}\n",
    "    except Exception as e:\n",
    "        return {\"question\": original_question, \"results\": f\"Error executing Cypher query: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94b0d9",
   "metadata": {},
   "source": [
    "## Define the prompt template for the final answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329909d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Based on the following question and query results, provide a concise and clear answer.\n",
    "If no results were found, state that.\n",
    "\n",
    "Question: {question}\n",
    "Query Results: {results}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e70dfa",
   "metadata": {},
   "source": [
    "## Build the LangChain pipeline\n",
    "The initial input to the chain is {\"question\": \"...\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5bccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypher_generation_chain = (\n",
    "    RunnablePassthrough.assign(schema=lambda x: schema) # Inject schema\n",
    "    | cypher_generation_prompt\n",
    "    | llm\n",
    "    | RunnableLambda(cypher_parser.parse) # Parse raw LLM output\n",
    "    | RunnableLambda(lambda cypher: add_limit_to_cypher(cypher, limit=6)) # Add limit\n",
    ")\n",
    "\n",
    "# Combine the Cypher generation with query execution and final answer generation\n",
    "# We need to preserve the original question throughout the chain for the final answer prompt.\n",
    "full_pipeline = (\n",
    "    # Step 1: Pass the initial question through and generate Cypher\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough.assign(\n",
    "        cypher_query=cypher_generation_chain.with_config(run_name=\"Generate Cypher\") # Run the first part of the chain\n",
    "    )\n",
    "    # Step 2: Execute the Cypher query. This step receives {\"question\": \"...\", \"cypher_query\": \"...\"}\n",
    "    | RunnableLambda(execute_and_format_cypher_query).with_config(run_name=\"Execute Query\") # Output is {\"question\": \"...\", \"results\": \"...\"}\n",
    "    # Step 3: Generate the final answer using the question and results\n",
    "    | final_answer_prompt\n",
    "    | llm_final_answer # Use the potentially different LLM for final answer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299c572",
   "metadata": {},
   "source": [
    "# USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009a2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: Show me titles of papers written in Spanish?\n",
      "\n",
      "Executing Cypher: MATCH (p:Paper)-[:WRITTEN_IN]->(l:Language) WHERE l.code = 'es' RETURN p.title LIMIT 6\n",
      "Query results: {'p.title': \"Ein Schluss für Rudolfs 'Alexander'? Überlegungen zum Cgm 203\"}\n",
      "{'p.title': 'Crítica, dialéctica y utopía'}\n",
      "{'p.title': 'Das Überraschende: Wittgenstein sobre o surpreendente em Matemática'}\n",
      "{'p.title': 'La versión aramaica del profeta Nahum'}\n",
      "{'p.title': 'Einstein, Gödel, Heidegger. Algunas consideraciones sobre el concepto del tiempo'}\n",
      "{'p.title': 'La patata: producción, comercialización e industria'}\n",
      "\n",
      "Final Answer from LLM: \n",
      "The following papers were written in Spanish:\n",
      "- Ein Schluss für Rudolfs 'Alexander'? Überlegungen zum Cgm 203\n",
      "- Crítica, dialéctica y utopía\n",
      "- Das Überraschende: Wittgenstein sobre o surpreendente em Matemática\n",
      "- La versión aramaica del profeta Nahum\n",
      "- Einstein, Gödel, Heidegger. Algunas consideraciones sobre el concepto del tiempo\n",
      "- La patata: producción, comercialización e industria\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Test the new full pipeline\n",
    "test_question = \"Show me titles of papers written in Spanish?\"\n",
    "print(f\"Original Question: {test_question}\")\n",
    "final_result = full_pipeline.invoke({\"question\": test_question})\n",
    "print(\"\\nFinal Answer from LLM:\", final_result)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537905ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: Show me titles of papers written in English?\n",
      "\n",
      "Executing Cypher: MATCH (p:Paper)-[:WRITTEN_IN]->(l:Language) WHERE l.code = 'en' RETURN p.title LIMIT 6\n",
      "Query results: {'p.title': 'Power play online: : Exploring master suppression techniques on Facebook'}\n",
      "{'p.title': 'Soil groups of Western Australia : a simple guide to the main soils of Western Australia (2nd rev ed - superseded by 4th ed)'}\n",
      "{'p.title': 'Precursor solution for polyimide/silica composite material, its manufacture method, and polymide/silica composite material having low volume shrinkage'}\n",
      "{'p.title': 'Vertical Track Stiffness as a New Parameter Involved in Designing High-Speed Railway Infrastructure'}\n",
      "{'p.title': 'Lowering of brachial pulse pressure in 9379 hypertensives with type 2 diabetes and reduction of cardiovascular events'}\n",
      "{'p.title': 'Supporting students with disabilities'}\n",
      "\n",
      "Final Answer from LLM: \n",
      "The titles of papers written in English are: 'Power play online: : Exploring master suppression techniques on Facebook', 'Soil groups of Western Australia : a simple guide to the main soils of Western Australia (2nd rev ed - superseded by 4th ed)', 'Precursor solution for polyimide/silica composite material, its manufacture method, and polymide/silica composite material having low volume shrinkage', 'Vertical Track Stiffness as a New Parameter Involved in Designing High-Speed Railway Infrastructure', 'Lowering of brachial pulse pressure in 9379 hypertensives with type 2 diabetes and reduction of cardiovascular events' and 'Supporting students with disabilities'.\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Test the new full pipeline\n",
    "test_question = \"Show me titles of papers written in English?\"\n",
    "print(f\"Original Question: {test_question}\")\n",
    "final_result = full_pipeline.invoke({\"question\": test_question})\n",
    "print(\"\\nFinal Answer from LLM:\", final_result)\n",
    "print('end')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
